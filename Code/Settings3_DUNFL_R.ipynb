{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"hVgMtb58ZIv7"},"outputs":[],"source":["#%%\n","import torch\n","import torch.nn as nn\n","from torchvision import datasets, transforms\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import csv\n","\n","#%% Parameters\n","T = 100                 # number of rounds for FL\n","M = 1                # number of learning iterations\n","Es = [2, 2, 2, 2, 2]    # number of epochs\n","batch_size = 50         # mini-batch size\n","lr_du = 0.001           # learning rate for proposed preprocess\n","mu = 0.01               # learning rate for ClientUpdate\n","num_feature = 128       # dimension of hidden layers\n","q = 1                   # parameter for DR-FedAvg\n","com_prob = [0.2, 0.3, 0.8, 0.9, 1]  # communication probability\n","\n","# Fixed parameters\n","K = 5                   # number of clients\n","datalocation = '/content/drive/MyDrive/Colab Notebooks/DeepUnfoldingFL/mnist/5clients_env2_3/'                     # data for env4\n","N = sum([1713, 1713, 1713, 1713, 1716])                     # number of all training data for env4\n","\n","print(\"Set parameters\", flush=True)\n","\n","#%% Global Data Load\n","root = '.'\n","download = True\n","trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))])\n","test_set = datasets.MNIST(root=root, train=False, transform=trans, download=download)\n","test_loader = torch.utils.data.DataLoader(dataset=test_set, batch_size=batch_size, shuffle=False)\n","\n","print(\"Download done\", flush=True)\n","\n","#%% Local Data\n","datasizes_test = []\n","for i in range(K):\n","    datasizes_test.append(int(round(len(test_set)/K)))\n","test_sets = torch.utils.data.random_split(dataset=test_set, lengths=datasizes_test, generator=torch.Generator().manual_seed(42))\n","\n","test_loaders = []\n","train_datasets = []\n","\n","for node in range (K):\n","    localtraindata = np.load(datalocation+'train/'+str(node)+'.npz', allow_pickle=True)     # training data load\n","    localtraindata = np.atleast_1d(localtraindata['data'])\n","    inputs = localtraindata[0]['x']\n","    targets = localtraindata[0]['y']\n","    tensor_X = torch.stack([torch.from_numpy(i) for i in inputs])\n","    tensor_y = torch.stack([torch.from_numpy(np.array(i)) for i in targets])\n","    train_datasets.append(torch.utils.data.TensorDataset(tensor_X,tensor_y))\n","    test_loaders.append(torch.utils.data.DataLoader(dataset=test_sets[node], batch_size=batch_size, shuffle=False))\n","pk = [len(train_datasets[node])/N for node in range(K)]\n","\n","print(\"Set local data\", flush=True)\n","\n","\n","#%% Definition of Initial Network\n","class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        self.l1 = nn.Linear(784, num_feature)\n","        self.l2 = nn.Linear(num_feature, num_feature)\n","        self.l3 = nn.Linear(num_feature, 10)\n","    def forward(self, x):\n","        x = torch.relu(self.l1(x))\n","        x = torch.relu(self.l2(x))\n","        x = self.l3(x)\n","        return F.log_softmax(x, dim=1)\n","\n","\n","#%% Definition of Unfolded FL\n","class TrainDUW(nn.Module):\n","    def __init__(self) -> None:\n","        super(TrainDUW, self).__init__()\n","        self.thetak = nn.ParameterList([nn.Parameter(torch.ones(T)*np.sqrt(len(train_datasets[i])/N)) for i in range(K)])\n","        # initial value: N_k/N\n","    def network(self, W1, b1, W2, b2, W3, b3, x):\n","        x = torch.relu(torch.matmul(x,W1.T)+b1.T)\n","        x = torch.relu(torch.matmul(x,W2.T)+b2.T)\n","        x = F.log_softmax(torch.matmul(x,W3.T)+b3.T, dim=1)\n","        return x\n","    def forward(self, aveW1, aveb1, aveW2, aveb2, aveW3, aveb3):\n","        outputlists = []\n","        targetlists = []\n","        for itr in range(T+1):\n","            tmploss = 0.\n","            outputlist = []\n","            targetlist = []\n","            iscom = torch.bernoulli(torch.Tensor(com_prob))\n","            if not itr == T:\n","                sumweights = sum([self.thetak[x][itr]**2 for x in range(K)])\n","            for node in range(K):\n","                weight1, bias1, weight2, bias2, weight3, bias3 = aveW1, aveb1, aveW2, aveb2, aveW3, aveb3\n","                weight1.requires_grad_(True)\n","                bias1.requires_grad_(True)\n","                weight2.requires_grad_(True)\n","                bias2.requires_grad_(True)\n","                weight3.requires_grad_(True)\n","                bias3.requires_grad_(True)\n","                train_loader_node = torch.utils.data.DataLoader(dataset=train_datasets[node], batch_size=batch_size, shuffle=True)\n","                outputvalues = []\n","                targetvalues = []\n","                for ep in range(Es[node]):\n","                    for (input, target) in train_loader_node:\n","                        input = input.view(-1, 28*28)\n","                        output = self.network(weight1,bias1,weight2,bias2,weight3,bias3,input)\n","                        outputvalues.append(output)\n","                        targetvalues.append(target)\n","                        if int(iscom[node]):\n","                            if not itr == T:\n","                                loss = F.nll_loss(output, target)\n","                                tmploss += loss.item()\n","                                w1grad = torch.autograd.grad(loss,weight1,retain_graph=True)\n","                                b1grad = torch.autograd.grad(loss,bias1,retain_graph=True)\n","                                w2grad = torch.autograd.grad(loss,weight2,retain_graph=True)\n","                                b2grad = torch.autograd.grad(loss,bias2,retain_graph=True)\n","                                w3grad = torch.autograd.grad(loss,weight3,retain_graph=True)\n","                                b3grad = torch.autograd.grad(loss,bias3,retain_graph=True)\n","                                weight1 = weight1 - mu * w1grad[0].detach()\n","                                bias1 = bias1 - mu * b1grad[0].detach()\n","                                weight2 = weight2 - mu * w2grad[0].detach()\n","                                bias2 = bias2 - mu * b2grad[0].detach()\n","                                weight3 = weight3 - mu * w3grad[0].detach()\n","                                bias3 = bias3 - mu * b3grad[0].detach()\n","                if not itr == T:\n","                    if node == 0:\n","                        weight0 = self.thetak[node][itr]**2 / sumweights\n","                        W1, b1, W2, b2, W3, b3 = weight1*weight0, bias1*weight0, weight2*weight0, bias2*weight0, weight3*weight0, bias3*weight0\n","                    else:\n","                        weightnode = self.thetak[node][itr]**2 / sumweights\n","                        W1, b1, W2, b2, W3, b3 = W1 + weight1*weightnode, b1 + bias1*weightnode, W2 + weight2*weightnode, b2 + bias2*weightnode, W3 + weight3*weightnode, b3 + bias3*weightnode\n","                outputlist.append(outputvalues)\n","                targetlist.append(targetvalues)\n","                if itr == T:\n","                    print(self.thetak[node]**2/sumweights, flush=True)\n","            aveW1, aveb1, aveW2, aveb2, aveW3, aveb3 = W1, b1, W2, b2, W3, b3\n","            outputlists.append(outputlist)\n","            targetlists.append(targetlist)\n","        return outputlists, targetlists\n","\n","print(\"Network defined\", flush=True)\n","\n","\n","#%% Functions\n","def model_initialize(initmodel):\n","    models = []\n","    for i in range(K):\n","        models.append(Net())\n","    for node in range(K):\n","        models[node].l1.weight.data = initmodel.l1.weight.data.clone()\n","        models[node].l1.bias.data = initmodel.l1.bias.data.clone()\n","        models[node].l2.weight.data = initmodel.l2.weight.data.clone()\n","        models[node].l2.bias.data = initmodel.l2.bias.data.clone()\n","        models[node].l3.weight.data = initmodel.l3.weight.data.clone()\n","        models[node].l3.bias.data = initmodel.l3.bias.data.clone()\n","    return models\n","\n","def train(datasets,model,optimizer,loop,method,modelfortheta,iscom):\n","    running_loss = 0.0\n","    if method=='dr':\n","        Fk = [0. for _ in range(K)]\n","    for node in range(K):\n","        train_loader_node = torch.utils.data.DataLoader(dataset=datasets[node], batch_size=batch_size, shuffle=True)\n","        for ep in range(Es[node]):\n","            for (input, target) in train_loader_node:\n","                input = input.view(-1, 28*28)\n","                optimizer[node].zero_grad()\n","                output = model[node](input)\n","                loss = F.nll_loss(output, target)\n","                if int(iscom[loop][node]):\n","                    loss.backward()\n","                    optimizer[node].step()\n","                running_loss += loss.item()\n","                if method=='dr':\n","                    Fk[node] += loss.item()\n","    # aggregation\n","    with torch.no_grad():\n","        if method=='du':\n","            sumweights = sum([modelfortheta.thetak[x][loop].item()**2 for x in range(K)])\n","        elif method=='dr':\n","            sumpkFk = sum([pk[node]*(Fk[node]**(q+1)) for node in range(K)])\n","        ps_global = []\n","        for params in range(len(list(model[0].parameters()))):\n","            if method=='du':\n","                ps_global.append(list(model[0].parameters())[params].data * modelfortheta.thetak[0][loop].data.item()**2 / sumweights)\n","            elif method=='origin':\n","                ps_global.append(list(model[0].parameters())[params].data*len(train_datasets[0])/N)\n","            elif method=='dr':\n","                ps_global.append(list(model[0].parameters())[params].data*pk[0]*(Fk[0]**(q+1))/sumpkFk)\n","            for node in range(1,K):\n","                if method=='du':\n","                    ps_global[params] += list(model[node].parameters())[params].data * modelfortheta.thetak[node][loop].data.item()**2 / sumweights\n","                elif method=='origin':\n","                    ps_global[params] += list(model[node].parameters())[params].data*len(train_datasets[node])/N\n","                elif method=='dr':\n","                    ps_global[params] += list(model[node].parameters())[params].data*pk[node]*(Fk[node]**(q+1))/sumpkFk\n","        # parameter sharing\n","        for node in range(K):\n","            model[node].l1.weight.data = ps_global[0].clone()\n","            model[node].l1.bias.data = ps_global[1].clone()\n","            model[node].l2.weight.data = ps_global[2].clone()\n","            model[node].l2.bias.data = ps_global[3].clone()\n","            model[node].l3.weight.data = ps_global[4].clone()\n","            model[node].l3.bias.data = ps_global[5].clone()\n","    return running_loss/K\n","\n","def test(dataloaders,model):\n","    correct =  0\n","    count = 0\n","    with torch.no_grad():\n","        for node in range(K):\n","            for (input, target) in dataloaders[node]:\n","                input = input.view(-1, 28*28)\n","                output = model[node](input)\n","                pred = output.argmax(dim=1)\n","                correct += pred.eq(target.data).sum()\n","                count += input.size()[0]\n","    return float(correct)/float(count)\n","\n","\n","#%% Model Sharing\n","model = Net()       # common initial model\n","models = model_initialize(model)        # models for proposed DUW-FedAvg\n","models2 = model_initialize(model)       # models for original FedAvg\n","models3 = model_initialize(model)       # models for DR-FedAvg\n","\n","# initial model parameters\n","aveW1 = model.l1.weight.data.clone().requires_grad_(False)\n","aveb1 = model.l1.bias.data.clone().requires_grad_(False)\n","aveW2 = model.l2.weight.data.clone().requires_grad_(False)\n","aveb2 = model.l2.bias.data.clone().requires_grad_(False)\n","aveW3 = model.l3.weight.data.clone().requires_grad_(False)\n","aveb3 = model.l3.bias.data.clone().requires_grad_(False)\n","\n","print(\"Model sharing done\", flush=True)\n","\n","\n","#%%\n","import time\n","\n","# Start the timer\n","start_time = time.time()\n","modelDU = TrainDUW()\n","learnedweights = torch.zeros(M+1, T, K)\n","sumweights = torch.zeros(T)\n","for itr in range(T):\n","    sumweights[itr] = sum([modelDU.thetak[x][itr].item()**2 for x in range(K)])\n","for node in range(K):\n","    learnedweights[0,:,node] = modelDU.thetak[node].detach()**2 / sumweights\n","optimizerDU = optim.Adam(modelDU.parameters(), lr=lr_du)\n","\n","#%%\n","# Training of Deep Unfolding-based Weights\n","outerloss = []\n","i = 0\n","for loop in range(M):\n","    i = i+1\n","    print(i, flush=True)\n","    optimizerDU.zero_grad()\n","    outputlists, targetlists = modelDU(aveW1, aveb1, aveW2, aveb2, aveW3, aveb3)\n","    loss = 0\n","    for j in range(T+1):\n","        for node in range(K):\n","            num_localdata = len(outputlists[j][node])\n","            for l in range(num_localdata):\n","                loss += F.nll_loss(outputlists[j][node][l], targetlists[j][node][l])\n","    loss.backward()\n","    optimizerDU.step()\n","    outerloss.append(loss.item())\n","    sumweights = torch.zeros(T)\n","    for itr in range(T):\n","        sumweights[itr] = sum([modelDU.thetak[x][itr].item()**2 for x in range(K)])\n","    for node in range(K):\n","        learnedweights[i,:,node] = modelDU.thetak[node].detach()**2 / sumweights\n","# Calculate the elapsed time\n","elapsed_time = time.time() - start_time\n","\n","# Print the elapsed time\n","print(\"Execution time:\", elapsed_time, \"seconds\")\n","print(\"Deep unfolding done\", flush=True)\n","# print ('outerloss = ', outerloss, flush=True)\n","\n","\n","#%% Federated Learning\n","iscom = [torch.bernoulli(torch.Tensor(com_prob)) for i in range(T)]\n","\n","# DUW-FedAvg\n","optimizers = []\n","for i in range(K):\n","    optimizers.append(optim.SGD(models[i].parameters(), lr=mu))\n","fl_loss = []\n","fl_acc = []\n","for loop in range(T):\n","    loss = train(train_datasets,models,optimizers,loop,'du',modelDU,iscom)\n","    fl_loss.append(loss)\n","    acc = test(test_loaders,models)\n","    fl_acc.append(acc)\n","\n","print(\"DUW-FedAvg done\", flush=True)\n","print ('DUW-FedAvg loss = ', fl_loss, flush=True)\n","print ('DUW-FedAvg acc = ', fl_acc, flush=True)\n","\n","#================================\n","#Save this results in csv file\n","# Saving results to a CSV file\n","filename = \"resultsEnv3_DUW.csv\"\n","\n","# Creating a list of dictionaries for each result entry\n","results = []\n","for i in range(len(fl_loss)):\n","    result = {\n","        'Iteration': i + 1,\n","        'Loss': fl_loss[i],\n","        'Accuracy': fl_acc[i]\n","    }\n","    results.append(result)\n","\n","# Writing the results to the CSV file\n","keys = ['Iteration', 'Loss', 'Accuracy']\n","\n","with open(filename, 'w', newline='') as file:\n","    writer = csv.DictWriter(file, fieldnames=keys)\n","    writer.writeheader()\n","    writer.writerows(results)\n","\n","print(f'Results saved to {filename}')\n","#================================\n","#%%\n","# original FedAvg\n","optimizers2 = []\n","for i in range(K):\n","    optimizers2.append(optim.SGD(models2[i].parameters(), lr=mu))\n","fl2_loss = []\n","fl2_acc = []\n","for loop in range(T):\n","    loss = train(train_datasets,models2,optimizers2,loop,'origin',modelDU,iscom)\n","    fl2_loss.append(loss)\n","    acc = test(test_loaders,models2)\n","    fl2_acc.append(acc)\n","\n","print(\"original FedAvg done\", flush=True)\n","print ('original FedAvg loss = ', fl2_loss, flush=True)\n","print ('original FedAvg acc = ', fl2_acc, flush=True)\n","\n","#%%\n","# DR-FedAvg\n","optimizers3 = []\n","for i in range(K):\n","    optimizers3.append(optim.SGD(models3[i].parameters(), lr=mu))\n","\n","# learning\n","fl3_loss = []\n","fl3_acc = []\n","for loop in range(T):\n","    loss = train(train_datasets,models3,optimizers3,loop,'dr',modelDU,iscom)\n","    fl3_loss.append(loss)\n","    acc = test(test_loaders,models3)\n","    fl3_acc.append(acc)\n","\n","print(\"DR-FedAvg done\", flush=True)\n","print ('DR-FedAvg loss = ', fl3_loss, flush=True)\n","print ('DR-FedAvg acc = ', fl3_acc, flush=True)\n","\n","'''\n","#%%\n","plt.figure()\n","plt.plot(fl_loss, linewidth=3, marker=\"o\", markersize=12, label=\"DUW-FedAvg\")\n","plt.plot(fl3_loss, linewidth=3, marker=\"x\", markersize=12, label=\"DR-FedAvg\")\n","plt.plot(fl2_loss, linewidth=3, marker=\"+\", markersize=12, label=\"FedAvg\")\n","plt.legend(fontsize=18)\n","plt.xlabel(\"round t\", fontsize=16)\n","plt.ylabel(\"loss\", fontsize=16)\n","plt.tick_params(labelsize=16)\n","plt.tight_layout()\n","plt.savefig(\"flloss.png\")\n","\n","#%%\n","plt.figure()\n","plt.plot(fl_acc, linewidth=3, marker=\"o\", markersize=12, label=\"DUW-FedAvg\")\n","plt.plot(fl3_acc, linewidth=3, marker=\"x\", markersize=12, label=\"DR-FedAvg\")\n","plt.plot(fl2_acc, linewidth=3, marker=\"+\", markersize=12, label=\"FedAvg\")\n","plt.legend(fontsize=18)\n","plt.xlabel(\"round t\", fontsize=16)\n","plt.ylabel(\"accuracy\", fontsize=16)\n","plt.tick_params(labelsize=16)\n","plt.tight_layout()\n","plt.savefig(\"flacc.png\")\n","'''"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1687407328477,"user":{"displayName":"Shanika Nanayakkara","userId":"01896911983542550879"},"user_tz":-600},"id":"iPSsUr3U9jjj","outputId":"8d056ca7-fcae-47af-e49f-6a44b25af195"},"outputs":[{"name":"stdout","output_type":"stream","text":["Results saved to learned_thetakEnv1org.csv\n"]}],"source":["#Save these theta results into csv files\n","import csv\n","import numpy as np\n","\n","K = 5\n","colors = ['red', 'blue', 'green', 'orange', 'purple']  # Specify the desired colors for each line\n","markers = ['o', 's', '^', 'v', 'D']  # Specify the desired markers for each line\n","\n","# Learned thetak\n","results = []\n","for node in range(K):\n","    labelname = 'learned theta ' + str(node)\n","    data = [i for i in learnedweights[M, :, node]]\n","    results.append([labelname] + data)\n","\n","# Save results to a CSV file\n","filename = \"learned_thetakEnv1org.csv\"\n","with open(filename, 'w', newline='') as csvfile:\n","    writer = csv.writer(csvfile)\n","    writer.writerow([\"Label\"] + [f\"Data {i+1}\" for i in range(len(data))])  # Write header row\n","    writer.writerows(results)  # Write data rows\n","\n","print(f\"Results saved to {filename}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x6yXQbUQ9meP"},"outputs":[],"source":["#Plot this same as other figure.\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","K = 5\n","colors = ['red', 'blue', 'green', 'orange', 'purple']  # Specify the desired colors for each line\n","markers = ['o', 's', '^', 'v', 'D']  # Specify the desired markers for each line\n","\n","# Learned thetak\n","fig2 = plt.figure()\n","for node in range(K):\n","    labelname = 'learned theta ' + str(node)\n","    plt.plot([i for i in learnedweights[M, :, node]], label=labelname, linewidth=3, marker=markers[node], markersize=12, color=colors[node])\n","plt.legend(fontsize=18)\n","plt.xlabel(\"com_round t\", fontsize=16)\n","plt.ylabel(\"learned theta\", fontsize=16)\n","plt.tick_params(labelsize=16)\n","plt.tight_layout()\n","fig2.savefig(\"learned_thetakEnv1Prop.png\")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"X05TLve0RDaU"},"source":["## Proposed code with Weight_decay =0.001"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rgnb0GeITs8V"},"outputs":[],"source":["\n","#%% this is for sgd optmizer to train unfolding\n","import torch\n","import torch.nn as nn\n","from torchvision import datasets, transforms\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import csv\n","\n","#%% Parameters\n","T = 10                  # number of rounds for FL\n","M = 100                # number of learning iterations\n","Es = [2, 2, 2, 2, 2]    # number of epochs\n","batch_size = 50         # mini-batch size\n","lr_du = 0.001           # learning rate for proposed preprocess\n","mu = 0.01               # learning rate for ClientUpdate\n","num_feature = 128       # dimension of hidden layers\n","q = 1                   # parameter for DR-FedAvg\n","com_prob = [0.2, 0.3, 0.8, 0.9, 1]  # communication probability\n","\n","# Fixed parameters\n","K = 5                   # number of clients\n","datalocation = '/content/drive/MyDrive/Colab Notebooks/DeepUnfoldingFL/mnist/5clients_env3_4/'                     # data for env4\n","N = sum([1713, 1713, 1713, 1713, 1716])                     # number of all training data for env4\n","\n","print(\"Set parameters\", flush=True)\n","\n","#%% Global Data Load\n","root = '.'\n","download = True\n","trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))])\n","test_set = datasets.MNIST(root=root, train=False, transform=trans, download=download)\n","test_loader = torch.utils.data.DataLoader(dataset=test_set, batch_size=batch_size, shuffle=False)\n","\n","print(\"Download done\", flush=True)\n","\n","#%% Local Data\n","datasizes_test = []\n","for i in range(K):\n","    datasizes_test.append(int(round(len(test_set)/K)))\n","test_sets = torch.utils.data.random_split(dataset=test_set, lengths=datasizes_test, generator=torch.Generator().manual_seed(42))\n","\n","test_loaders = []\n","train_datasets = []\n","\n","for node in range (K):\n","    localtraindata = np.load(datalocation+'train/'+str(node)+'.npz', allow_pickle=True)     # training data load\n","    localtraindata = np.atleast_1d(localtraindata['data'])\n","    inputs = localtraindata[0]['x']\n","    targets = localtraindata[0]['y']\n","    tensor_X = torch.stack([torch.from_numpy(i) for i in inputs])\n","    tensor_y = torch.stack([torch.from_numpy(np.array(i)) for i in targets])\n","    train_datasets.append(torch.utils.data.TensorDataset(tensor_X,tensor_y))\n","    test_loaders.append(torch.utils.data.DataLoader(dataset=test_sets[node], batch_size=batch_size, shuffle=False))\n","pk = [len(train_datasets[node])/N for node in range(K)]\n","\n","print(\"Set local data\", flush=True)\n","\n","\n","#%% Definition of Initial Network\n","class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        self.l1 = nn.Linear(784, num_feature)\n","        self.l2 = nn.Linear(num_feature, num_feature)\n","        self.l3 = nn.Linear(num_feature, 10)\n","    def forward(self, x):\n","        x = torch.relu(self.l1(x))\n","        x = torch.relu(self.l2(x))\n","        x = self.l3(x)\n","        return F.log_softmax(x, dim=1)\n","\n","\n","#%% Definition of Unfolded FL\n","class TrainDUW(nn.Module):\n","    def __init__(self) -> None:\n","        super(TrainDUW, self).__init__()\n","        self.thetak = nn.ParameterList([nn.Parameter(torch.ones(T)*np.sqrt(len(train_datasets[i])/N)) for i in range(K)])\n","        # initial value: N_k/N\n","    def network(self, W1, b1, W2, b2, W3, b3, x):\n","        x = torch.relu(torch.matmul(x,W1.T)+b1.T)\n","        x = torch.relu(torch.matmul(x,W2.T)+b2.T)\n","        x = F.log_softmax(torch.matmul(x,W3.T)+b3.T, dim=1)\n","        return x\n","    def forward(self, aveW1, aveb1, aveW2, aveb2, aveW3, aveb3):\n","        outputlists = []\n","        targetlists = []\n","        for itr in range(T+1):\n","            tmploss = 0.\n","            outputlist = []\n","            targetlist = []\n","            iscom = torch.bernoulli(torch.Tensor(com_prob))\n","            if not itr == T:\n","                sumweights = sum([self.thetak[x][itr]**2 for x in range(K)])\n","            for node in range(K):\n","                weight1, bias1, weight2, bias2, weight3, bias3 = aveW1, aveb1, aveW2, aveb2, aveW3, aveb3\n","                weight1.requires_grad_(True)\n","                bias1.requires_grad_(True)\n","                weight2.requires_grad_(True)\n","                bias2.requires_grad_(True)\n","                weight3.requires_grad_(True)\n","                bias3.requires_grad_(True)\n","                train_loader_node = torch.utils.data.DataLoader(dataset=train_datasets[node], batch_size=batch_size, shuffle=True)\n","                outputvalues = []\n","                targetvalues = []\n","                for ep in range(Es[node]):\n","                    for (input, target) in train_loader_node:\n","                        input = input.view(-1, 28*28)\n","                        output = self.network(weight1,bias1,weight2,bias2,weight3,bias3,input)\n","                        outputvalues.append(output)\n","                        targetvalues.append(target)\n","                        if int(iscom[node]):\n","                            if not itr == T:\n","                                loss = F.nll_loss(output, target)\n","                                tmploss += loss.item()\n","                                w1grad = torch.autograd.grad(loss,weight1,retain_graph=True)\n","                                b1grad = torch.autograd.grad(loss,bias1,retain_graph=True)\n","                                w2grad = torch.autograd.grad(loss,weight2,retain_graph=True)\n","                                b2grad = torch.autograd.grad(loss,bias2,retain_graph=True)\n","                                w3grad = torch.autograd.grad(loss,weight3,retain_graph=True)\n","                                b3grad = torch.autograd.grad(loss,bias3,retain_graph=True)\n","                                weight1 = weight1 - mu * w1grad[0].detach()\n","                                bias1 = bias1 - mu * b1grad[0].detach()\n","                                weight2 = weight2 - mu * w2grad[0].detach()\n","                                bias2 = bias2 - mu * b2grad[0].detach()\n","                                weight3 = weight3 - mu * w3grad[0].detach()\n","                                bias3 = bias3 - mu * b3grad[0].detach()\n","                if not itr == T:\n","                    if node == 0:\n","                        weight0 = self.thetak[node][itr]**2 / sumweights\n","                        W1, b1, W2, b2, W3, b3 = weight1*weight0, bias1*weight0, weight2*weight0, bias2*weight0, weight3*weight0, bias3*weight0\n","                    else:\n","                        weightnode = self.thetak[node][itr]**2 / sumweights\n","                        W1, b1, W2, b2, W3, b3 = W1 + weight1*weightnode, b1 + bias1*weightnode, W2 + weight2*weightnode, b2 + bias2*weightnode, W3 + weight3*weightnode, b3 + bias3*weightnode\n","                outputlist.append(outputvalues)\n","                targetlist.append(targetvalues)\n","                if itr == T:\n","                    print(self.thetak[node]**2/sumweights, flush=True)\n","            aveW1, aveb1, aveW2, aveb2, aveW3, aveb3 = W1, b1, W2, b2, W3, b3\n","            outputlists.append(outputlist)\n","            targetlists.append(targetlist)\n","        return outputlists, targetlists\n","\n","print(\"Network defined\", flush=True)\n","\n","\n","#%% Functions\n","def model_initialize(initmodel):\n","    models = []\n","    for i in range(K):\n","        models.append(Net())\n","    for node in range(K):\n","        models[node].l1.weight.data = initmodel.l1.weight.data.clone()\n","        models[node].l1.bias.data = initmodel.l1.bias.data.clone()\n","        models[node].l2.weight.data = initmodel.l2.weight.data.clone()\n","        models[node].l2.bias.data = initmodel.l2.bias.data.clone()\n","        models[node].l3.weight.data = initmodel.l3.weight.data.clone()\n","        models[node].l3.bias.data = initmodel.l3.bias.data.clone()\n","    return models\n","\n","def train(datasets,model,optimizer,loop,method,modelfortheta,iscom):\n","    running_loss = 0.0\n","    if method=='dr':\n","        Fk = [0. for _ in range(K)]\n","    for node in range(K):\n","        train_loader_node = torch.utils.data.DataLoader(dataset=datasets[node], batch_size=batch_size, shuffle=True)\n","        for ep in range(Es[node]):\n","            for (input, target) in train_loader_node:\n","                input = input.view(-1, 28*28)\n","                optimizer[node].zero_grad()\n","                output = model[node](input)\n","                loss = F.nll_loss(output, target)\n","                if int(iscom[loop][node]):\n","                    loss.backward()\n","                    optimizer[node].step()\n","                running_loss += loss.item()\n","                if method=='dr':\n","                    Fk[node] += loss.item()\n","    # aggregation\n","    with torch.no_grad():\n","        if method=='du':\n","            sumweights = sum([modelfortheta.thetak[x][loop].item()**2 for x in range(K)])\n","        elif method=='dr':\n","            sumpkFk = sum([pk[node]*(Fk[node]**(q+1)) for node in range(K)])\n","        ps_global = []\n","        for params in range(len(list(model[0].parameters()))):\n","            if method=='du':\n","                ps_global.append(list(model[0].parameters())[params].data * modelfortheta.thetak[0][loop].data.item()**2 / sumweights)\n","            elif method=='origin':\n","                ps_global.append(list(model[0].parameters())[params].data*len(train_datasets[0])/N)\n","            elif method=='dr':\n","                ps_global.append(list(model[0].parameters())[params].data*pk[0]*(Fk[0]**(q+1))/sumpkFk)\n","            for node in range(1,K):\n","                if method=='du':\n","                    ps_global[params] += list(model[node].parameters())[params].data * modelfortheta.thetak[node][loop].data.item()**2 / sumweights\n","                elif method=='origin':\n","                    ps_global[params] += list(model[node].parameters())[params].data*len(train_datasets[node])/N\n","                elif method=='dr':\n","                    ps_global[params] += list(model[node].parameters())[params].data*pk[node]*(Fk[node]**(q+1))/sumpkFk\n","        # parameter sharing\n","        for node in range(K):\n","            model[node].l1.weight.data = ps_global[0].clone()\n","            model[node].l1.bias.data = ps_global[1].clone()\n","            model[node].l2.weight.data = ps_global[2].clone()\n","            model[node].l2.bias.data = ps_global[3].clone()\n","            model[node].l3.weight.data = ps_global[4].clone()\n","            model[node].l3.bias.data = ps_global[5].clone()\n","    return running_loss/K\n","\n","def test(dataloaders,model):\n","    correct =  0\n","    count = 0\n","    with torch.no_grad():\n","        for node in range(K):\n","            for (input, target) in dataloaders[node]:\n","                input = input.view(-1, 28*28)\n","                output = model[node](input)\n","                pred = output.argmax(dim=1)\n","                correct += pred.eq(target.data).sum()\n","                count += input.size()[0]\n","    return float(correct)/float(count)\n","\n","\n","#%% Model Sharing\n","model = Net()       # common initial model\n","models = model_initialize(model)        # models for proposed DUW-FedAvg\n","models2 = model_initialize(model)       # models for original FedAvg\n","models3 = model_initialize(model)       # models for DR-FedAvg\n","\n","# initial model parameters\n","aveW1 = model.l1.weight.data.clone().requires_grad_(False)\n","aveb1 = model.l1.bias.data.clone().requires_grad_(False)\n","aveW2 = model.l2.weight.data.clone().requires_grad_(False)\n","aveb2 = model.l2.bias.data.clone().requires_grad_(False)\n","aveW3 = model.l3.weight.data.clone().requires_grad_(False)\n","aveb3 = model.l3.bias.data.clone().requires_grad_(False)\n","\n","print(\"Model sharing done\", flush=True)\n","\n","\n","#%%\n","import time\n","\n","# Start the timer\n","start_time = time.time()\n","modelDU = TrainDUW()\n","learnedweights = torch.zeros(M+1, T, K)\n","sumweights = torch.zeros(T)\n","for itr in range(T):\n","    sumweights[itr] = sum([modelDU.thetak[x][itr].item()**2 for x in range(K)])\n","for node in range(K):\n","    learnedweights[0,:,node] = modelDU.thetak[node].detach()**2 / sumweights\n","optimizerDU = optim.SGD(modelDU.parameters(), lr=lr_du,weight_decay=0.001)\n","\n","#%%,weight_decay=0.01\n","# Training of Deep Unfolding-based Weights\n","outerloss = []\n","i = 0\n","for loop in range(M):\n","    i = i+1\n","    print(i, flush=True)\n","    optimizerDU.zero_grad()\n","    outputlists, targetlists = modelDU(aveW1, aveb1, aveW2, aveb2, aveW3, aveb3)\n","    loss = 0\n","    for j in range(T+1):\n","        for node in range(K):\n","            num_localdata = len(outputlists[j][node])\n","            for l in range(num_localdata):\n","                loss += F.nll_loss(outputlists[j][node][l], targetlists[j][node][l])\n","    loss.backward()\n","    optimizerDU.step()\n","    outerloss.append(loss.item())\n","    sumweights = torch.zeros(T)\n","    for itr in range(T):\n","        sumweights[itr] = sum([modelDU.thetak[x][itr].item()**2 for x in range(K)])\n","    for node in range(K):\n","        learnedweights[i,:,node] = modelDU.thetak[node].detach()**2 / sumweights\n","# Calculate the elapsed time\n","elapsed_time = time.time() - start_time\n","\n","# Print the elapsed time\n","print(\"Execution time:\", elapsed_time, \"seconds\")\n","print(\"Deep unfolding done\", flush=True)\n","# print ('outerloss = ', outerloss, flush=True)\n","\n","\n","#%% Federated Learning\n","iscom = [torch.bernoulli(torch.Tensor(com_prob)) for i in range(T)]\n","\n","# DUWR-FedAvg\n","optimizers = []\n","for i in range(K):\n","  #optim.Adam(modelDU.parameters(), lr=lr_du,weight_decay=0.01\n","    optimizers.append(optim.SGD(models[i].parameters(), lr=mu,weight_decay=0.001))\n","fl_loss = []\n","fl_acc = []\n","for loop in range(T):\n","    loss = train(train_datasets,models,optimizers,loop,'du',modelDU,iscom)\n","    fl_loss.append(loss)\n","    acc = test(test_loaders,models)\n","    fl_acc.append(acc)\n","\n","print(\"DUW-FedAvg done\", flush=True)\n","print ('DUW-FedAvg loss = ', fl_loss, flush=True)\n","print ('DUW-FedAvg acc = ', fl_acc, flush=True)\n","\n","# Saving results to a CSV file\n","filename = \"resultsEnv3_DUWR.csv\"\n","\n","# Creating a list of dictionaries for each result entry\n","results = []\n","for i in range(len(fl_loss)):\n","    result = {\n","        'Iteration': i + 1,\n","        'Loss': fl_loss[i],\n","        'Accuracy': fl_acc[i]\n","    }\n","    results.append(result)\n","\n","# Writing the results to the CSV file\n","keys = ['Iteration', 'Loss', 'Accuracy']\n","\n","with open(filename, 'w', newline='') as file:\n","    writer = csv.DictWriter(file, fieldnames=keys)\n","    writer.writeheader()\n","    writer.writerows(results)\n","\n","print(f'Results saved to {filename}')\n","#%%\n","# original FedAvg\n","optimizers2 = []\n","for i in range(K):\n","    optimizers2.append(optim.SGD(models2[i].parameters(), lr=mu,weight_decay=0.001))\n","fl2_loss = []\n","fl2_acc = []\n","for loop in range(T):\n","    loss = train(train_datasets,models2,optimizers2,loop,'origin',modelDU,iscom)\n","    fl2_loss.append(loss)\n","    acc = test(test_loaders,models2)\n","    fl2_acc.append(acc)\n","\n","print(\"original FedAvg done\", flush=True)\n","print ('original FedAvg loss = ', fl2_loss, flush=True)\n","print ('original FedAvg acc = ', fl2_acc, flush=True)\n","\n","# Saving results to a CSV file\n","filename = \"resultsEnv3_FedAvg.csv\"\n","\n","# Creating a list of dictionaries for each result entry\n","results = []\n","for i in range(len(fl_loss)):\n","    result = {\n","        'Iteration': i + 1,\n","        'Loss': fl_loss[i],\n","        'Accuracy': fl_acc[i]\n","    }\n","    results.append(result)\n","\n","# Writing the results to the CSV file\n","keys = ['Iteration', 'Loss', 'Accuracy']\n","\n","with open(filename, 'w', newline='') as file:\n","    writer = csv.DictWriter(file, fieldnames=keys)\n","    writer.writeheader()\n","    writer.writerows(results)\n","\n","print(f'Results saved to {filename}')\n","#%%\n","# DR-FedAvg\n","optimizers3 = []\n","for i in range(K):\n","    optimizers3.append(optim.SGD(models3[i].parameters(), lr=mu,weight_decay=0.001))\n","\n","# learning\n","fl3_loss = []\n","fl3_acc = []\n","for loop in range(T):\n","    loss = train(train_datasets,models3,optimizers3,loop,'dr',modelDU,iscom)\n","    fl3_loss.append(loss)\n","    acc = test(test_loaders,models3)\n","    fl3_acc.append(acc)\n","\n","print(\"DR-FedAvg done\", flush=True)\n","print ('DR-FedAvg loss = ', fl3_loss, flush=True)\n","print ('DR-FedAvg acc = ', fl3_acc, flush=True)\n","\n","# Saving results to a CSV file\n","filename = \"resultsEnv3_DR.csv\"\n","\n","# Creating a list of dictionaries for each result entry\n","results = []\n","for i in range(len(fl_loss)):\n","    result = {\n","        'Iteration': i + 1,\n","        'Loss': fl_loss[i],\n","        'Accuracy': fl_acc[i]\n","    }\n","    results.append(result)\n","\n","# Writing the results to the CSV file\n","keys = ['Iteration', 'Loss', 'Accuracy']\n","\n","with open(filename, 'w', newline='') as file:\n","    writer = csv.DictWriter(file, fieldnames=keys)\n","    writer.writeheader()\n","    writer.writerows(results)\n","\n","print(f'Results saved to {filename}')\n","\n"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":376,"status":"ok","timestamp":1687407862969,"user":{"displayName":"Shanika Nanayakkara","userId":"01896911983542550879"},"user_tz":-600},"id":"YKt2waBmF0Ld","outputId":"9bea0b57-cc35-43b4-bfa2-ddabe9f0e54f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Results saved to learned_thetakEnv4_propose.csv\n"]}],"source":["# Learned thetak\n","results = []\n","for node in range(K):\n","    labelname = 'learned theta ' + str(node)\n","    data = [i for i in learnedweights[M, :, node]]\n","    results.append([labelname] + data)\n","\n","# Save results to a CSV file\n","filename = \"learned_thetakEnv4_propose.csv\"\n","with open(filename, 'w', newline='') as csvfile:\n","    writer = csv.writer(csvfile)\n","    writer.writerow([\"Label\"] + [f\"Data {i+1}\" for i in range(len(data))])  # Write header row\n","    writer.writerows(results)  # Write data rows\n","\n","print(f\"Results saved to {filename}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oJDlRpdhF4HP"},"outputs":[],"source":["#Plot this same as other figure.\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","K = 5\n","colors = ['red', 'blue', 'green', 'orange', 'purple']  # Specify the desired colors for each line\n","markers = ['o', 's', '^', 'v', 'D']  # Specify the desired markers for each line\n","\n","# Learned thetak\n","fig2 = plt.figure()\n","for node in range(K):\n","    labelname = 'learned theta ' + str(node)\n","    plt.plot([i for i in learnedweights[M, :, node]], label=labelname, linewidth=3, marker=markers[node], markersize=12, color=colors[node])\n","plt.legend(fontsize=18)\n","plt.xlabel(\"com_round t\", fontsize=16)\n","plt.ylabel(\"learned theta\", fontsize=16)\n","plt.tick_params(labelsize=16)\n","plt.tight_layout()\n","fig2.savefig(\"learned_thetak.png\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fzRfI8F-ccWE"},"outputs":[],"source":["#code for environment 01\n","import matplotlib.pyplot as plt\n","import csv\n","# Define the file path\n","filename = \"resultsEnv3_DUWR.csv\"\n","\n","# Create empty lists to store the data\n","iterations = []\n","Proposed_DUWFedAvgloss = []\n","Proposed_DUWFedAvgacc = []\n","\n","# Read the CSV file\n","with open(filename, 'r') as file:\n","    reader = csv.DictReader(file)\n","    for row in reader:\n","        iterations.append(int(row['Iteration']))\n","        Proposed_DUWFedAvgloss.append(float(row['Loss']))\n","        Proposed_DUWFedAvgacc.append(float(row['Accuracy']))\n","\n","# Print the retrieved data\n","#print(\"Iterations:\", iterations)\n","print(\"Proposed_DUWFedAvgloss Losses:\", Proposed_DUWFedAvgloss)\n","print(\"Proposed_DUWFedAvgacc Accuracies:\", Proposed_DUWFedAvgacc)\n","#==================================\n","# Define the file path\n","filename = \"resultsEnv3_DUW.csv\"\n","\n","# Create empty lists to store the data\n","iterations = []\n","DUWFedAvgloss = []\n","DUWFedAvgacc = []\n","\n","# Read the CSV file\n","with open(filename, 'r') as file:\n","    reader = csv.DictReader(file)\n","    for row in reader:\n","        iterations.append(int(row['Iteration']))\n","        DUWFedAvgloss.append(float(row['Loss']))\n","        DUWFedAvgacc.append(float(row['Accuracy']))\n","\n","# Print the retrieved data\n","#print(\"Iterations:\", iterations)\n","print(\"DUWFedAvgloss Losses:\", DUWFedAvgloss)\n","print(\"DUWFedAvgacc Accuracies:\", DUWFedAvgacc)\n","\n","#===========================\n","\n","# Define the file path\n","filename = \"resultsEnv3_FedAvg.csv\"\n","\n","# Create empty lists to store the data\n","iterations = []\n","originalFedAvgloss = []\n","originalFedAvgacc = []\n","\n","# Read the CSV file\n","with open(filename, 'r') as file:\n","    reader = csv.DictReader(file)\n","    for row in reader:\n","        iterations.append(int(row['Iteration']))\n","        originalFedAvgloss.append(float(row['Loss']))\n","        originalFedAvgacc.append(float(row['Accuracy']))\n","\n","# Print the retrieved data\n","#print(\"Iterations:\", iterations)\n","print(\"originalFedAvgloss Losses:\", originalFedAvgloss)\n","print(\"originalFedAvgacc Accuracies:\", originalFedAvgacc)\n","#============================\n","\n","# Define the file path\n","filename = \"resultsEnv3_DR.csv\"\n","\n","# Create empty lists to store the data\n","iterations = []\n","DRFedAvgloss = []\n","DRFedAvgacc = []\n","\n","# Read the CSV file\n","with open(filename, 'r') as file:\n","    reader = csv.DictReader(file)\n","    for row in reader:\n","        iterations.append(int(row['Iteration']))\n","        DRFedAvgloss.append(float(row['Loss']))\n","        DRFedAvgacc.append(float(row['Accuracy']))\n","\n","# Print the retrieved data\n","#print(\"Iterations:\", iterations)\n","print(\"DRFedAvgloss Losses:\", DRFedAvgloss)\n","print(\"DRFedAvgacc Accuracies:\", DRFedAvgacc)\n","\n","plt.figure()\n","plt.plot(Proposed_DUWFedAvgloss, linewidth=3, marker=\"o\", markersize=12,color='orange', label=\"DUW-FedAvg_Proposed\")\n","plt.plot(DUWFedAvgloss, linewidth=3, marker=\"o\", markersize=12,color='blue', label=\"DUW-FedAvg\")\n","plt.plot(DRFedAvgloss, linewidth=3, marker=\"x\", markersize=12,color='green', label=\"DR-FedAvg\")\n","plt.plot(originalFedAvgloss, linewidth=3, marker=\"+\", markersize=12,color='red', label=\"FedAvg\")\n","plt.legend(fontsize=18)\n","plt.xlabel(\"com_round t\", fontsize=16)\n","plt.ylabel(\"loss\", fontsize=16)\n","plt.tick_params(labelsize=16)\n","plt.tight_layout()\n","plt.savefig(\"Propsed_loss.png\")\n","\n","#%%\n","plt.figure()\n","plt.plot(Proposed_DUWFedAvgacc, linewidth=3, marker=\"o\", markersize=12,color='orange', label=\"DUW-FedAvg_Proposed\")\n","plt.plot(DUWFedAvgacc, linewidth=3, marker=\"o\", markersize=12,color='blue', label=\"DUW-FedAvg\")\n","\n","plt.plot(DRFedAvgacc, linewidth=3, marker=\"x\", markersize=12,color='green', label=\"DR-FedAvg\")\n","plt.plot(originalFedAvgacc, linewidth=3, marker=\"+\", markersize=12,color='red', label=\"FedAvg\")\n","plt.legend(fontsize=18)\n","plt.xlabel(\"com_round t\", fontsize=16)\n","plt.ylabel(\"accuracy\", fontsize=16)\n","plt.tick_params(labelsize=16)\n","plt.tight_layout()\n","plt.savefig(\"Proposed_acc.png\")"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyNgLPpJ4z9kl6/hrBNS5uDM","gpuType":"T4","machine_shape":"hm","mount_file_id":"1v_n6gWFxud1miGhSG_QNdRxCum4Z4LjW","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
